{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SpotifyDelta.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO06csaWYSqU1jEx+xdU7/a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tobiasz-talaj/Spotify-Delta/blob/main/SpotifyDelta.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRlsXjZSICAg"
      },
      "source": [
        "## Configuration and imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNPgookZHv6L"
      },
      "source": [
        "# Install dependencies\r\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\r\n",
        "!wget -q http://mirror.its.dal.ca/apache/spark/spark-2.4.7/spark-2.4.7-bin-hadoop2.7.tgz # Check https://downloads.apache.org/spark/ for current version\r\n",
        "!tar xvf spark-2.4.7-bin-hadoop2.7.tgz\r\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjdY3_AqJT8X"
      },
      "source": [
        "# Set environment path\r\n",
        "import os\r\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\r\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.3.2-bin-hadoop2.7\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fPouwrFJZ_I"
      },
      "source": [
        "# Use findspark to make pyspark importable as a regular library, import modules\r\n",
        "import findspark\r\n",
        "findspark.init(\"spark-2.4.7-bin-hadoop2.7\")\r\n",
        "from pyspark.sql import SQLContext, SparkSession\r\n",
        "from pyspark.sql import functions as F\r\n",
        "from pyspark.sql.window import Window\r\n",
        "from google.colab import drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hx1Nun41RsTl",
        "outputId": "6b77ed55-47a6-49b7-f2f0-257b3f57eef9"
      },
      "source": [
        "# Connect Colab to your Google Drive\r\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNw236hzL2dT"
      },
      "source": [
        "# Define path where to find files\r\n",
        "root_path = 'gdrive/My Drive/Vectra Assignment/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TndfH5_IT-E"
      },
      "source": [
        "## Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jd7CqV0KKQVV"
      },
      "source": [
        "# Set up Spark session\r\n",
        "sc = SparkSession.builder.master(\"local[*]\").getOrCreate()\r\n",
        "sqlc = SQLContext(sc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glIR3jq-uAHp"
      },
      "source": [
        "# Load data, create and slice dataframe\r\n",
        "data_by_year = sc.read.option(\"header\", True).csv(root_path+'data_by_year.csv', encoding='utf-8')\r\n",
        "data_by_year = data_by_year[['year', 'energy', 'danceability']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwdfIQQ5MBtQ"
      },
      "source": [
        "# Create function adding 'previous value' and 'delta' columns to original dataframe\r\n",
        "def add_prev_value_and_delta(df):\r\n",
        "  df = df.withColumn(\"prev_value_energy\", F.lag(df.energy).over(my_window))\r\n",
        "  df = df.withColumn(\"prev_value_danceability\", F.lag(df.danceability).over(my_window))\r\n",
        "  df = df.withColumn(\"delta_energy\", F.when(F.isnull(df.energy - df.prev_value_energy), 0).otherwise(df.energy - df.prev_value_energy))\r\n",
        "  df = df.withColumn(\"delta_danceability\", F.when(F.isnull(df.danceability - df.prev_value_danceability), 0).otherwise(df.danceability - df.prev_value_danceability))\r\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GryzKzOOO8pZ",
        "outputId": "e65d5c93-737d-43b4-c2c9-f4608321e3e9"
      },
      "source": [
        "# Create window with partitioning and show the results\r\n",
        "my_window = Window.partitionBy().orderBy(\"year\")\r\n",
        "add_prev_value_and_delta(data_by_year).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+-------------------+-------------------+-------------------+-----------------------+--------------------+--------------------+\n",
            "|year|             energy|       danceability|  prev_value_energy|prev_value_danceability|        delta_energy|  delta_danceability|\n",
            "+----+-------------------+-------------------+-------------------+-----------------------+--------------------+--------------------+\n",
            "|1920|0.41869957020057297|  0.515750143266476|               null|                   null|                 0.0|                 0.0|\n",
            "|1921| 0.2411363461538462|  0.432170512820513|0.41869957020057297|      0.515750143266476|-0.17756322404672678|-0.08357963044596295|\n",
            "|1922|0.22617264462809922| 0.5756198347107437| 0.2411363461538462|      0.432170512820513|-0.01496370152574697| 0.14344932189023074|\n",
            "|1923| 0.2624064864864865| 0.5773405405405401|0.22617264462809922|     0.5756198347107437|0.036233841858387295|0.001720705829796...|\n",
            "|1924| 0.3443466101694912| 0.5498940677966102| 0.2624064864864865|     0.5773405405405401| 0.08194012368300468|-0.02744647274392986|\n",
            "|1925| 0.2780860215053763| 0.5748745519713262| 0.3443466101694912|     0.5498940677966102|-0.06626058866411488| 0.02498048417471599|\n",
            "|1926|0.21050642588412327| 0.5969149736644092| 0.2780860215053763|     0.5748745519713262|-0.06757959562125304|0.022040421693082957|\n",
            "|1927|0.26790213333333335| 0.6559293333333329|0.21050642588412327|     0.5969149736644092|0.057395707449210076|0.059014359668923766|\n",
            "|1928| 0.2088558196078431| 0.5349066666666661|0.26790213333333335|     0.6559293333333329|-0.05904631372549024|-0.12102266666666683|\n",
            "|1929|0.24190778128285972| 0.6478398527865401| 0.2088558196078431|     0.5349066666666661| 0.03305196167501662| 0.11293318611987402|\n",
            "|1930|0.33485984093319204| 0.5197101802757159|0.24190778128285972|     0.6478398527865401| 0.09295205965033232| -0.1281296725108242|\n",
            "|1931|0.23473644938016547| 0.5948927685950423|0.33485984093319204|     0.5197101802757159|-0.10012339155302658| 0.07518258831932634|\n",
            "|1932|0.27836227621483395| 0.6022140664961636|0.23473644938016547|     0.5948927685950423| 0.04362582683466848|0.007321297901121349|\n",
            "|1933| 0.2867542735042735| 0.5700692307692304|0.27836227621483395|     0.6022140664961636|0.008391997289439568|-0.03214483572693316|\n",
            "|1934| 0.2635702749140894|  0.529168384879725| 0.2867542735042735|     0.5700692307692304|-0.02318399859018...|-0.04090084588950549|\n",
            "|1935|0.24662291479820622|  0.554997053171045| 0.2635702749140894|      0.529168384879725|-0.01694736011588...|0.025828668291320023|\n",
            "|1936| 0.2640076749500002| 0.6233780500000001|0.24662291479820622|      0.554997053171045|0.017384760151793993| 0.06838099682895515|\n",
            "|1937|0.31051245083207246| 0.5426403933434191| 0.2640076749500002|     0.6233780500000001| 0.04650477588207225|-0.08073765665658106|\n",
            "|1938| 0.2812483922829584|0.48051045016077204|0.31051245083207246|     0.5426403933434191|-0.02926405854911407|-0.06212994318264703|\n",
            "|1939| 0.2848278000000002| 0.5136558000000002| 0.2812483922829584|    0.48051045016077204|0.003579407717041...| 0.03314534983922818|\n",
            "+----+-------------------+-------------------+-------------------+-----------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XRGMEH2PF1_"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7cW_KSpwTry"
      },
      "source": [
        "testing_dataframe_0 = sc.createDataFrame([(1937, 0.31051245083207246, 0.5426403933434191), (1938, 0.2812483922829584, 0.48051045016077204),], ['year', 'energy', 'danceability'])\r\n",
        "testing_dataframe_1 = sc.createDataFrame([(1, 1, 1), (2, 2, 2),], ['year', 'energy', 'danceability']) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpnG3vUNhnWd"
      },
      "source": [
        "import unittest\r\n",
        "\r\n",
        "class FunctionalityTests(unittest.TestCase):\r\n",
        "    \r\n",
        "  def test_prev_value_a(self):\r\n",
        "    tdf_0 = add_prev_value_and_delta(testing_dataframe_0)\r\n",
        "    self.assertIsNone(tdf_0.where(tdf_0.year == 1937).select('prev_value_energy').collect()[0]['prev_value_energy'])\r\n",
        "    self.assertEqual(tdf_0.where(tdf_0.year == 1938).select('prev_value_energy').collect()[0]['prev_value_energy'], 0.31051245083207246)\r\n",
        "    self.assertIsNone(tdf_0.where(tdf_0.year == 1937).select('prev_value_danceability').collect()[0]['prev_value_danceability'])\r\n",
        "    self.assertEqual(tdf_0.where(tdf_0.year == 1938).select('prev_value_danceability').collect()[0]['prev_value_danceability'], 0.5426403933434191)\r\n",
        "\r\n",
        "  def test_prev_value_b(self):\r\n",
        "    tdf_1 = add_prev_value_and_delta(testing_dataframe_1)\r\n",
        "    self.assertIsNone(tdf_1.where(tdf_1.year == 1).select('prev_value_energy').collect()[0]['prev_value_energy'])\r\n",
        "    self.assertEqual(tdf_1.where(tdf_1.year == 2).select('prev_value_energy').collect()[0]['prev_value_energy'], 1)\r\n",
        "    self.assertIsNone(tdf_1.where(tdf_1.year == 1).select('prev_value_danceability').collect()[0]['prev_value_danceability'])\r\n",
        "    self.assertEqual(tdf_1.where(tdf_1.year == 2).select('prev_value_danceability').collect()[0]['prev_value_danceability'], 1)\r\n",
        "\r\n",
        "  def test_delta_a(self):\r\n",
        "    tdf_0 = add_prev_value_and_delta(testing_dataframe_0)\r\n",
        "    self.assertEqual(tdf_0.where(tdf_0.year == 1937).select('delta_energy').collect()[0]['delta_energy'], 0)\r\n",
        "    self.assertEqual(tdf_0.where(tdf_0.year == 1938).select('delta_energy').collect()[0]['delta_energy'], -0.02926405854911407)\r\n",
        "    self.assertEqual(tdf_0.where(tdf_0.year == 1937).select('delta_danceability').collect()[0]['delta_danceability'], 0)\r\n",
        "    self.assertEqual(tdf_0.where(tdf_0.year == 1938).select('delta_danceability').collect()[0]['delta_danceability'], -0.06212994318264703)\r\n",
        "\r\n",
        "  def test_delta_b(self):\r\n",
        "    tdf_1 = add_prev_value_and_delta(testing_dataframe_1)\r\n",
        "    self.assertEqual(tdf_1.where(tdf_1.year == 1).select('delta_energy').collect()[0]['delta_energy'], 0)\r\n",
        "    self.assertEqual(tdf_1.where(tdf_1.year == 2).select('delta_energy').collect()[0]['delta_energy'], 1)\r\n",
        "    self.assertEqual(tdf_1.where(tdf_1.year == 1).select('delta_danceability').collect()[0]['delta_danceability'], 0)\r\n",
        "    self.assertEqual(tdf_1.where(tdf_1.year == 2).select('delta_danceability').collect()[0]['delta_danceability'], 1)\r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "    unittest.main(argv=['arg'], exit=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}